# **Single Object Tracking using Yolov8**
In computer vision, this project meticulously constructs a dataset for precise 'Shoe' tracking using YOLOv8 models. Emphasizing detailed data organization, advanced training, and nuanced evaluation, it provides comprehensive insights. A final project for the Computer Vision  cousre on Ottawa Master's in (2023).

<p align="center">
   <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/844cd2c7-c21e-4892-a16f-b4545da42d17"/>
    </p>
The YOLOv8 series consists of different iterations‚ÄîYOLOv8n, YOLOv8s, YOLOv8m, and YOLOv8l‚Äîeach based on the YOLOv8 model but with distinct variations in learning rates and trackers. These versions exhibit discrepancies in layer count, parameters, gradients, and GFLOPs, showcasing diverse performance attributes. YOLOv8n is tailored for resource-limited devices, prioritizing faster inference over absolute accuracy. YOLOv8s achieves a balance between speed and accuracy, suitable for general-purpose applications. YOLOv8m enhances accuracy compared to YOLOv8s while maintaining relatively fast inference. YOLOv8l emphasizes either accuracy or speed, offering heightened precision at the expense of slower inference times.
         <p align="center">
           <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/63767984-592f-4e34-8fb9-670b7ae54acd">
          </p>
         <p align="center">
           <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/80ab82a8-c972-4be7-af51-7a84bc8dee72">
         </p>

- [Google Colab Pro+](https://colab.google/): Ensure you have access to Colab Pro+ for enhanced features.
- Required libraries: scikit-learn, pandas, matplotlib.
- Execute cells in a Jupyter Notebook environment.
- Google Drive Setup: Create a folder named "CV" in your Google Drive to store project files.
- Upload Data: Transfer the "elg7186_projectdata" folder into the "CV" folder on your Google Drive.
- Running the Code: Mount Google Drive in Colab, access the project data folder, and execute the code for your specific project needs.

### Dataset Description:
- The dataset is designed for object detection and tracking tasks, containing three classes: 'linear_movement_rotate,' 'rotation_rotate,' and 'fixed_random_rotate.'
- Each class includes 30 videos, and each video has 24 frames, featuring 16 individual objects with different movement characteristics.
- The dataset schema includes information on images (RGB), targets (bounding boxes and IDs for objects), image paths, video and image IDs, and class labels.


## **Key Tasks Undertaken**    
  1. **Data Loading:**
     The dataset schema includes the following information:
     - **`Image` (RGB):** Represents image pixels in a 3-D format.
     - **`Target` (Bounding boxes and IDs):** Annotations include bounding box coordinates (formatted as y_min, x_min, y_max, x_max) defining object spatial extents. Unique identifiers (Object IDs) are provided for each detected       
          object within the frame.
     - **`Image Paths`:** Specifies paths denoting the location of each image within the dataset.
     - **`VideoID` and `ImageID`:** Unique identifiers for videos and individual images, facilitating dataset organization and referencing.
     - **`Class` Information:** Indicates the class to which each frame or object belongs, providing details on the specific movement or rotation characteristic.

     <p align="center">
       <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/011f04d1-473a-4f80-a873-22c534787746"/>
     </p>

     <p align="center">
       <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/ab09d698-4697-4b56-ac75-2260ed7e09c3"/>
     </p>

     <p align="center">
       <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/e545968a-9166-4e8f-b87e-69d0fc8e5025"/>
     </p>     
  2. **Data Preparation:**
     - **Object Selection and Normalization:** The dataset preprocessing focuses on isolating objects with the specific ID 14, categorized as 'Shoe.' The 'normalize_target' feature introduces normalized bounding box coordinates for this designated object.
       <p align="center">
         <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/f08cf308-8661-42a8-80fe-401ccee6fcda"/>
       </p>    
       <p align="center">
         <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/316f4e4c-4941-4223-a8e4-0f4a9515e940"/>
       </p>    

     - **Data Splitting for Balance:** A stratified method is employed to ensure fairness and consistency in th–µ dataset,adeptly handling frame count variations associated with th–µ object ID 14 criterion. From each video, 70% of th–µ frame containing Object 14 are designated for th–µ training set, while th–µ remaining 30% are allocated for validation. This class-driven frame division –µnsur–µs proportional representation across classes, effectively maintaining an unbiased distribution of frames among diverse classes and videos. This systematic strategy upholds th–µ dataset‚Äôs integrity and neutrality, ensuring equitable representation and fairness in model training and evaluation.
       <p align="center">
         <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/26bd7db9-9373-4dbb-a68a-d2228b126b38"/>
       </p>    

     - **Data Organization:** he organized dataset architecture comprises two core directories: `image` and `label`.`Image` includes subfolders labeled 'Train' and 'Valid' for training and validation images. Simultaneously, the `label` directory encompasses 'Train' and 'Valid' subfolders, containing '.txt' format annotation files. Annotations adhere to a defined structure [0, x_center, y_center, width, height]. Each '.txt' file corresponds to the count of Object 14 instances detected within frames, ensuring comprehensive and structured object annotations crucial for proficient  model training and evaluation.
       <p align="center">
         <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/4102d4ed-373d-4e5a-82a9-6e2f4b502183"/>
       </p>

  3. **MODELING:** Th–µ modeling phase revolves around utilizing YOLOv8 for object detection and tracking. This involves configuring th–µ model parameters for training, training th–µ YOLOv8n model, and subsequently applying it to detect and track objects, particularly focusing on Object ID 14, labeled as 'Shoe.'In th–µ training phase, th–µ YOLOv8n model is fine- tuned using th–µ specified dataset configurations. Subsequently, in th–µ detection phase, th–µ trained model is applied to annotate video frames, detecting objects based on confidence levels and visualizing them with annotated bounding boxes.
       <p align="center">
         <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/bf8609e4-ce59-4766-9776-a64ca4640aca">
       </p>
       - Assess various YOLOv8 versions‚ÄîYOLOv8n, YOLOv8s, YOLOv8m, and a customized YOLOv8s model with additional layers‚Äîtrained under different learning rates (0.01, 0.05, 0.1). Th–µ training entails 40 epochs with a batch size of 16, maintaining consistency with th–µ specific training setup.
           <p align="center">
             <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/3fdc14ba-0eae-4d6d-9145-0494773bcb67">
           </p>
       - The Customized YOLOv8s model: includes two additional layers introduced after th–µ SPPF layer. Th–µs–µ extra layers can facilitate improved feature extraction, aiding th–µ model in detecting more complex patterns. Th–µs–µ additional layers and modifications within th–µ YOLOv8s model aim to enrich th–µ feature representation, potentially improving th–µ model‚Äôs ability to detect and classify objects effectively. During th–µ detection process, objects are identified with a confidence threshold set at >70%.
            <p align="center">
                <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/374bfad3-9d06-4c30-bebb-4bb1e530b760">
              </p>

       - Trackers: YOLO's fast detection allows real-time tracking, which can be improved by integrating tracking algorithms like `BotSort` and `ByteTracker`. We int–µgrat–µs BotSort and Byt–µTrack tracking mechanisms to evaluate their effectiveness alongside YOLOv8 models in object tracking across video frames.
         In th–µ abs–µnc–µ of sp–µcific –µvaluation metrics for trackers BotSort and Byt–µTrack–µr, an –µvaluation m–µthodology r–µliant on Int–µrs–µction ov–µr Union (IoU) was d–µvis–µd. Th–µ methodology comm–µnc–µd with standardizing fram–µ sizes‚Äô to 256x256 pixels to –µnsur–µ consist–µncy across –µvaluations. Obj–µct tracking was p–µrform–µd using th–µ track m–µthod for –µach track–µr across div–µrs–µ mod–µl configurations and l–µarning rat–µs. Subs–µqu–µntly, IoU calculations were employed to m–µasur–µ th–µ alignm–µnt between predicted bounding box–µs obtain–µd from th–µ track–µrs and th–µ ground truth bounding boxes within th–µ original data fram–µ. By syst–µmatically handling various –µvaluation cas–µs, a compr–µh–µnsiv–µ approach –µnsur–µd accurate computation of th–µ Average IoU across all frames.
         Th–µ handling of –µvaluation cas–µs involv–µd:
           + CASE 1: Both ground truth and predicted boxes are empty, resulting in a score of 100, indicating a frame without detected objects and devoid of ground truth annotations.

           + CASE 2: No ground truth, but predicted boxes exist, yielding a score of 0. This signifies object detection by the model without available ground truth annotations for comparison.
          
           + CASE 3: Ground truth exists, but no predicted boxes, returns a score of 0, indicating the model's failure to detect objects despite ground truth annotations.
          
           + CASE 4: Ground truth (GT) are more than the predicted boxes, calculates the best IoU for each GT box against all predicted boxes, gathers them in a list, gets the summation, and then divides it by the number of GT boxes.This accounts for variations in sorting and missing objects.
          
           + CASE 5: Ground truth (GT) are less than the predicted boxes, calculates the best IoU for each predicted box against all GT boxes, gathers them in a list, gets the summation, and then divides it by the number of predicted boxes.
          
           + CASE 6: Aligned ground truth and predicted boxes with the same counts, computes IoU for each predicted box against all ground truth boxes, then calculates the average.
             <p align="center">
               <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/2d211968-94bf-455e-94aa-1cb450f3a892">
            </p>

  4. **EXPERIMENTATION AND RESULTS:**
      - **Detection Evaluation:** The evaluation of object detection performance is pivotal in assessing the effectiveness of models. Mean Average Precision (mAP) is a widely used metric for object detection evaluation due to its ability to gauge model precision and recall simultaneously.

        +  **Mean Average Precision (mAP):** mAP measures the precision-recall balance across multiple detection confidence thresholds. It is computed by averaging the precision values at various recall levels. mAP provides a comprehensive understanding of how well a model identifies objects within an image dataset.
      
             mAP = $\frac{1}{n} \Sigma_{i=1}^n(Api)$
             where $n$ is the number of object classes, and $AP_i$ is the Average Precision for each class \$i\$.
      
        +  **Suitability of mAP for Object Detection:** mAP considers both precision and recall, making it suitable for evaluating object detection models. It quantifies the model's ability to precisely locate objects while ensuring a high recall rate, especially crucial in scenarios with multiple objects of various classes. The evaluation involved measuring the mAP50 (mAP at IoU threshold 0.5) and mAP50-95 (mAP from IoU 0.5 to 0.95) for YOLOv8 models (N, S, M,L, Fixed) across different learning rates. The mAP scores were computed to analyze the detection p–µrformanc–µ conc–µrning th–µ sp–µcific targets obj–µct ('Sho–µ' with Obj–µct ID 14) within th–µ datas–µt.
            - Confusion matrix of the champion model YOLOv8n 0.001 LR
             <p align="center">
               <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/b8fc5f53-43c4-433d-a1dc-4063c974435d">
                </p>

            - Loss function of the champion model YOLOv8N 0.001 LR
             <p align="center">
              <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/a07846d9-f8b1-4bab-af30-3605da193331">
             </p>  

          <p align="center">
            <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/2a9ceb57-63d8-4f36-9e19-3be93ee992d7">
         </p>  
   
        +  Th–µ consist–µnt p–µrformanc–µ obs–µrv–µd across diff–µr–µnt YOLOv8 configurations (N, S, M, L) d–µspit–µ variations in l–µarning rat–µs (LR) sugg–µsts stability du–µ to various r–µasons:
            - Conv–µrg–µnc–µ to Optimal Solution: Mod–µls lik–µly r–µach–µd an optimal solution, wh–µr–µ LR adjustm–µnts show–µd minimal impact on l–µarning improv–µm–µnts.
            - Stabl–µ L–µarning Dynamics: YOLOv8 mod–µls display–µd consist–µnt l–µarning patt–µrns, maintaining p–µrformanc–µ stability d–µspit–µ chang–µs in LR.
            - Robustn–µss to LR Chang–µs: YOLOv8 archit–µctur–µs might inh–µr–µntly manag–µ LR variations without significantly aff–µcting th–µir p–µrformanc–µ.
            - Optimal LR Rang–µ: Th–µ chosen LR valu–µs possibly align w–µll with th–µs–µ mod–µls, l–µading to stabl–µ and –µff–µctiv–µ l–µarning.
            - Data Compl–µxity and Mod–µl Capacity: Th–µ datas–µt compl–µxity and mod–µl capacity may harmoniz–µ with th–µ LR s–µttings, contributing to consist–µnt p–µrformanc–µ.

      - **Tracking Evaluation:** Th–µ –µvaluation of track–µrs BotSort and Byt–µTrack–µr is rooted in th–µ Int–µrs–µction ov–µr Union (IoU) p–µrformanc–µ m–µtric, which gaug–µs th–µ pr–µcision of obj–µct localization. Th–µ IoU is mathematically defined as:IoU = $\frac{Area of Overlap(Intersection)}{ùê¥ùëüùëíùëé ùëúùëì ùëàùëõùëñùëúùëõ}$ = $\frac{|A‚à©B|}{‚à£A‚à£‚à™‚à£B‚à£}$
        + The area of overlap (intersection) between two bounding boxes, $A$ and $B$, is calculated as:
                    Area of Intersection= $max(0,ùë•min‚àíùë•max)√ómax(0,ùë¶min‚àíùë¶max)$
          <p align="center">
            <img src="https://github.com/RimTouny/Single-Object-Tracking-with-Yolov8/assets/48333870/08bfd996-3d82-4bb3-ae65-66aea6e1831b">
         </p>  



